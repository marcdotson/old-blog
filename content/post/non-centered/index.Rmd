---
title: "Non-centered parameterization for hierarchical models"
author: "Marc Dotson"
date: '2020-03-15'
slug: non-centered
---

In a [previous post](https://www.occasionaldivergences.com/post/stan-hierarchical/), we provided a gentle introduction to building hierarchical Bayesian models in Stan. We quickly ran into divergences (i.e., divergent transitions) when attempting to estimate our model. While hierarchical models have posteriors with [geometry that is difficult to navigate](https://arxiv.org/abs/1312.0906), we were able to initially fix the problem by more carefully navigating the posterior with a smaller step size.

However, the posterior geometry of hierarchical models typically requires a non-centered parameterization, expressing the same model in a different way to ease the navigation of the posterior. The goal of this post is to expand on that [previous post](https://www.occasionaldivergences.com/post/stan-hierarchical/) by providing a walkthrough of a non-centered parameterization for hierarchical Bayesian models in Stan.

## Non-centered parameterization

Our assumed error is `delta ~ normal(0, 1)` such that `beta = mu + tau * delta`. Following a centered parameterization, we can translate this into a population model and likelihood for a simple hierarchical regression as:

```
beta ~ normal(mu, tau)
y ~ normal(beta, sigma)
```

where we get draws from the posterior distribution of `mu`, `tau`, and `beta`.

On the other hand, following a non-centered parameterization, we can translate this into a population model and likelihood for a simple hierarchical regression as:

```
delta ~ normal(0, 1)
beta = mu + tau * delta
y ~ normal(beta, sigma)
```

where we get draws from the posterior distribution of `mu`, `tau`, and `delta`, since `beta` is now a fixed transformation of the other parameters. The benefit of a non-centered parameterization is including `delta` rather than `beta` in the posterior distribution since `beta` is often piece that complicates the posterior with difficult geometry.

## Simple hierarchical regression

Let's start by demonstrating how to implement a non-centered parameterization for a simple hierarchical regression.

```{stan simple-hlm, output.var="simple_hlm", eval=FALSE}
// Index values and observations.
data {
  int<lower = 1> N;               // Number of observations.
  int<lower = 1> K;               // Number of groups.
  vector[N] y;                    // Vector of observations.
  int<lower = 1, upper = K> g[N]; // Vector of group assignments.
}

// Parameters and hyperparameters.
parameters {
  real mu;                        // Mean of the population model.
  real<lower = 0> tau;            // Variance of the population model.
  vector[K] delta;                // Vector of non-centered group intercepts.
  real<lower = 0> sigma;          // Variance of the likelihood.
}

// Deterministic transformation.
transformed parameters {
  // Vector of transformed group intercepts.
  vector[K] beta;

  // Non-centered parameterization.
  for (k in 1:K) {
    beta[k] = mu + tau * delta[k];
  }
}

// Hierarchical regression.
model {
  // Hyperpriors.
  mu ~ normal(0, 5);
  tau ~ normal(0, 5);

  // Prior.
  sigma ~ normal(0, 5);

  // Non-centered population model and likelihood.
  for (k in 1:K) {
    delta[k] ~ normal(0, 1);
  }
  for (n in 1:N) {
    y[n] ~ normal(beta[g[n]], sigma);
  }
}
```

Instead of `beta`, we include `delta` in the `parameters` block, noted here as the non-centered group intercepts. To impose the deterministic transformation, we include a `transformed parameters` block, which includes both `beta` and the non-centered transformation itself `beta[k] = mu + tau * delta[k]`. Finally, in place of `beta`, the non-centered population model `delta[k] ~ normal(0, 1)` is included in the `model` block.

## Multiple hierarchical regression

- Add lower-level covariates `X`.
- Add upper-level covariates `Z`.

-----

### Marc Dotson

Marc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on [Twitter](https://twitter.com/marcdotson) and [GitHub](https://github.com/marcdotson).
