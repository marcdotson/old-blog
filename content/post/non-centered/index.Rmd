---
title: "Non-centered parameterization for hierarchical models"
author: "Marc Dotson"
date: '2020-05-15'
slug: non-centered
---

In a [previous post](https://www.occasionaldivergences.com/post/stan-hierarchical/), we provided a gentle introduction to hierarchical Bayesian models in Stan. We quickly ran into divergences (i.e., divergent transitions) when attempting to estimate our model. While hierarchical models inherently have posteriors with [geometry that can be difficult to navigate](https://arxiv.org/abs/1312.0906), we were able to initially address this problem by more carefully navigating the posterior with a smaller step size.

However, as a hierarchical model becomes more complicated, we need to re-express it in a way that is mathematically equivalent yet results in a posterior that is easier to navigate. We ended that previous post without relaxing the assumption of a common variance in the upper-level population model. In this post, we will build a hierarchical linear model with a multivariate population model. This added complexity will require us to re-express the model using what is known as a non-centered parameterization. Since this more flexible population model specification is standard, so should a non-centered parameterization be our default approach in most applications. Once again, I am in debt to Michael Betancourt's [case studies](https://betanalpha.github.io/writing/), Richard McElreath's [*Statistical Rethinking*](https://xcelab.net/rm/statistical-rethinking/), and the [Stan User's Guide](https://mc-stan.org/docs/2_23/stan-users-guide/index.html).

## Multivariate population model

- Motivate this model specification.
- Replace `tau` with a covariance matrix and draw `Beta` with a multivariate normal.

The motivation for hierarchical models was to allow there to be differences in groups, to strike the balance between no pooling and complete pooling. However, so far we have only allowed this for the means. What about the variances? What about the need for a covariance matrix? Assuming independence removes flexibility and serves as a much stricter assumption.

The standard deviation parameter adapts the amount of pooling across units...

```{stan hlm-centered, output.var="hlm_centered", eval=FALSE}

```

## Non-centered parameterization

The standard centered parameterization for a simple hierarchical regression has a population model and likelihood:

```
beta ~ normal(mu, tau)
y ~ normal(beta, sigma)
```

where we get draws from the posterior distribution of `mu` and `tau`, the population mean and variance, and `beta`, the group-level intercepts.

A non-centered parameterization re-expresses the population model and likelihood for a simple hierarchical regression as:

```
delta ~ normal(0, 1)
beta = mu + tau * delta
y ~ normal(beta, sigma)
```

where we get draws from the posterior distribution of `mu`, `tau`, and `delta`, since `beta` is now a fixed transformation of the other parameters (i.e., we have `beta =` instead of `beta ~`). The benefit of a non-centered parameterization -- the inclusion of the intermediate `delta` and the fixed transformation of `beta` -- is that difficult dependencies between the two layers in the hierarchy are broken, producing a simpler posterior geometry.

### Simple hierarchical regression

Let's start by demonstrating how to implement a non-centered parameterization for a simple hierarchical regression.

```{stan simple-hlm, output.var="simple_hlm", eval=FALSE}
// Index values and observations.
data {
  int<lower = 1> N;               // Number of observations.
  int<lower = 1> K;               // Number of groups.
  vector[N] y;                    // Vector of observations.
  int<lower = 1, upper = K> g[N]; // Vector of group assignments.
}

// Parameters and hyperparameters.
parameters {
  real mu;                        // Mean of the population model.
  real<lower = 0> tau;            // Variance of the population model.
  vector[K] delta;                // Vector of non-centered group intercepts.
  real<lower = 0> sigma;          // Variance of the likelihood.
}

// Deterministic transformation.
transformed parameters {
  // Vector of transformed group intercepts.
  vector[K] beta;

  // Non-centered parameterization.
  for (k in 1:K) {
    beta[k] = mu + tau * delta[k];
  }
}

// Hierarchical regression.
model {
  // Hyperpriors.
  mu ~ normal(0, 5);
  tau ~ normal(0, 5);

  // Prior.
  sigma ~ normal(0, 5);

  // Non-centered population model and likelihood.
  for (k in 1:K) {
    delta[k] ~ normal(0, 1);
  }
  for (n in 1:N) {
    y[n] ~ normal(beta[g[n]], sigma);
  }
}
```

Instead of `beta`, we include `delta` in the `parameters` block, noted here as the non-centered group intercepts. To impose the deterministic transformation, we include a `transformed parameters` block, which includes both `beta` and the non-centered transformation itself `beta[k] = mu + tau * delta[k]`. Finally, in place of `beta`, the non-centered population model `delta[k] ~ normal(0, 1)` is included in the `model` block.

### Multiple hierarchical regression

Note that the need to break difficult dependencies between the two layers in the hierarchy grows with thinner data or more dimensions introduced with covariates. However, the centered and non-centered parameterizations are inversely related in terms of efficiency; when a centered parameterization will suffice, a non-centered parameterization should underperform and when a non-centered parameterization will suffice, a centered parameterization should underperform.

- Respond to Stan Discourse reply and include variance parameters (reference Stan User's Guide).
- Plot Delta on tau to see where the divergences are appearing (may need to run longer chains to get better plots).

- Worry about indexing efficiency (i.e., arrays instead of matrices)? Pre-computing X'Beta with a local variable assignment? See Stan User's Guide on Hierarchical Logistic Regression.
- Look at implementation for HMNL?
- Under what conditions does NCP > CP? CP > NCP?
- Adding lower-level covariates `X` -- why does this break NCP?
- Adding upper-level covariates `Z` -- why does this break NCP?

-----

### Marc Dotson

Marc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on [Twitter](https://twitter.com/marcdotson) and [GitHub](https://github.com/marcdotson).
