---
title: "Non-centered parameterization for hierarchical models"
author: "Marc Dotson"
date: '2020-03-01'
slug: non-centered
---

In a [previous post](https://www.occasionaldivergences.com/post/stan-hierarchical/), we provided a gentle introduction to building hierarchical models in Stan. We quickly ran into divergences (i.e., divergent transitions) when attempting to estimate our model. While hierarchical models have posteriors with [geometry that is difficult to navigate](https://arxiv.org/abs/1312.0906), which is true independent of sampler, we were able to initially fix the problem by more carefully navigating the posterior with a smaller step size.

However, the posterior geometry of hierarchical models typically requires a non-centered parameterization, expressing the same model in a different way to ease the navigation of the posterior. The goal of this post is to expand on that [previous post](https://www.occasionaldivergences.com/post/stan-hierarchical/) by providing a walkthrough for non-centered parameterizations for hierarchical Bayesian models in Stan.

## Non-centered parameterization

Our assumed error is `delta ~ normal(0, 1)` such that `beta = mu + tau * delta`. Following a centered parameterization, we can translate this into a population model and likelihood for a simple hierarchical regression as:

```
beta ~ normal(mu, tau);
y ~ normal(beta, sigma);
```

where we get draws from the posterior distribution of `mu`, `tau`, and `beta`.

On the other hand, following a non-centered parameterization, we can translate this into a population model and likelihood for a simple hierarchical regression as:

```
delta ~ normal(0, 1);
beta = mu + tau * delta;
y ~ normal(beta, sigma);
```

where we now get draws from the posterior distribution of `mu`, `tau`, and `delta`, since `beta` is now a fixed transformation rather than its own draw. The non-centered parameterization is thus simply that, including `delta` rather than `beta` in the posterior distribution, where `beta` is often the vector of parameters that introduces difficult geometry.

## Simple hierarchical regression


## Multiple hierarchical regression


-----

### Marc Dotson

Marc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on [Twitter](https://twitter.com/marcdotson) and [GitHub](https://github.com/marcdotson).
