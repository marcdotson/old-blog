---
title: "Non-centered parameterization for hierarchical models"
author: "Marc Dotson"
date: '2020-05-15'
slug: non-centered
---

In a [previous post](https://www.occasionaldivergences.com/post/stan-hierarchical/), we provided a gentle introduction to hierarchical Bayesian models in Stan. We quickly ran into divergences (i.e., divergent transitions) when attempting to estimate our model. While hierarchical models inherently have [posteriors with geometry that can be difficult to navigate](https://arxiv.org/abs/1312.0906), we were able to initially address this problem by more carefully navigating the posterior with a smaller step size.

However, as a hierarchical model becomes more complicated, we need to re-express it in a way that is mathematically equivalent yet results in a posterior that is easier to navigate. We ended that previous post without relaxing the assumption of a common variance in the upper-level population model. In this post, we will build a hierarchical linear model with a multivariate population model. This added complexity will require us to re-express the model using what is known as a non-centered parameterization. Since this more flexible population model specification is standard, so should a non-centered parameterization be our default approach in most applications. Once again, I am in debt to Michael Betancourt's [case studies](https://betanalpha.github.io/writing/), Richard McElreath's [*Statistical Rethinking*](https://xcelab.net/rm/statistical-rethinking/), and the [Stan User's Guide](https://mc-stan.org/docs/2_23/stan-users-guide/index.html).

## Multivariate population model

Recall that the motivation for hierarchical models is to allow for differences across groups while employing partial pooling, striking a balance between no pooling and complete pooling. However, so far we have assumed a common variance in the population model, limiting how much partial information pooling is possible. By moving to a multivariate population model, we allow for more partial information pooling, where each group-level parameter probability can now be associated with every other group-level parameter probability.

Covariance matrices can be difficult to work with, especially when it comes to setting a prior (or, in our case, a hyperprior). A popular approach is to break down the covariance matrix into component pieces. Perhaps the most intuitive decomposition is to break down the covariance matrix into variances and a correlation matrix. If we have a covariance matrix `Sigma`, this decomposition works as follows:

```
Sigma = diag_matrix(tau) * Omega * diag_matrix(tau)
```

where `tau` is a vector of scale parameters and `Omega` is a correlation matrix. Not only is this decomposition computationally more tractable, but it allows us to set a prior on `tau` and `Omega` separately rather than on `Sigma` directly.

What does this model look like in Stan?

```{stan hlm-centered, output.var="hlm_centered", eval=FALSE}
// Index values, observations, and covariates.
data {
  int<lower = 1> N;               // Number of observations.
  int<lower = 1> K;               // Number of groups.
  int<lower = 1> I;               // Number of observation-level covariates.
  int<lower = 1> J;               // Number of population-level covariates.

  vector[N] y;                    // Vector of observations.
  int<lower = 1, upper = K> g[N]; // Vector of group assignments.
  matrix[N, I] X;                 // Matrix of observation-level covariates.
  matrix[K, J] Z;                 // Matrix of population-level covariates.
}

// Parameters and hyperparameters.
parameters {
  matrix[J, I] Gamma;             // Matrix of population-level coefficients.
  corr_matrix[I] Omega;           // Correlation matrix for the population model.
  vector<lower = 0>[I] tau;       // Vector of scale parameters for the population model.
  matrix[K, I] Beta;              // Matrix of observation-level coefficients.
  real<lower = 0> sigma;          // Variance of the likelihood.
}

// Hierarchical regression.
model {
  // Hyperpriors.
  for (j in 1:J) {
    Gamma[j,] ~ normal(0, 5);
  }
  Omega ~ lkj_corr(2);
  tau ~ normal(0, 5);

  // Prior.
  sigma ~ normal(0, 5);

  // Population model and likelihood.
  for (k in 1:K) {
    Beta[k,] ~ multi_normal(Z[k,] * Gamma, quad_form_diag(Omega, tau));
  }
  for (n in 1:N) {
    y[n] ~ normal(X[n,] * Beta[g[n],]', sigma);
  }
}
```

In the `parameters` block, we now have a correlation matrix `Omega` and `tau` is now a vector of scale parameters rather than a scalar. In the `models` block, `Omega` is distributed according to the LKJ distribution, a Beta-like distribution for correlation matrices. Additionally, `Beta` is now distributed multivariate normal, with the covariance matrix a recombination of `Omega` and `tau` as described above.

Since all Bayesain models are generative, we can translate this Stan script, `hlm_centered.stan` into `data` and `generated quantities` blocks called `generate_data.stan` to use Stan to generate data for us.

```{stan generate-data, output.var="generate_data", eval=FALSE}

```

Running this model gives us an excess of 4,000 divergent transitions...

## Non-centered parameterization

The standard centered parameterization for a simple hierarchical regression has a population model and likelihood:

```
beta ~ normal(mu, tau)
y ~ normal(beta, sigma)
```

where we get draws from the posterior distribution of `mu` and `tau`, the population mean and variance, and `beta`, the group-level intercepts.

A non-centered parameterization re-expresses the population model and likelihood for a simple hierarchical regression as:

```
delta ~ normal(0, 1)
beta = mu + tau * delta
y ~ normal(beta, sigma)
```

where we get draws from the posterior distribution of `mu`, `tau`, and `delta`, since `beta` is now a fixed transformation of the other parameters (i.e., we have `beta =` instead of `beta ~`). The benefit of a non-centered parameterization -- the inclusion of the intermediate `delta` and the fixed transformation of `beta` -- is that difficult dependencies between the two layers in the hierarchy are broken, producing a simpler posterior geometry.

### Simple hierarchical regression

Let's start by demonstrating how to implement a non-centered parameterization for a simple hierarchical regression.

```{stan simple-hlm, output.var="simple_hlm", eval=FALSE}
// Index values and observations.
data {
  int<lower = 1> N;               // Number of observations.
  int<lower = 1> K;               // Number of groups.
  vector[N] y;                    // Vector of observations.
  int<lower = 1, upper = K> g[N]; // Vector of group assignments.
}

// Parameters and hyperparameters.
parameters {
  real mu;                        // Mean of the population model.
  real<lower = 0> tau;            // Variance of the population model.
  vector[K] delta;                // Vector of non-centered group intercepts.
  real<lower = 0> sigma;          // Variance of the likelihood.
}

// Deterministic transformation.
transformed parameters {
  // Vector of transformed group intercepts.
  vector[K] beta;

  // Non-centered parameterization.
  for (k in 1:K) {
    beta[k] = mu + tau * delta[k];
  }
}

// Hierarchical regression.
model {
  // Hyperpriors.
  mu ~ normal(0, 5);
  tau ~ normal(0, 5);

  // Prior.
  sigma ~ normal(0, 5);

  // Non-centered population model and likelihood.
  for (k in 1:K) {
    delta[k] ~ normal(0, 1);
  }
  for (n in 1:N) {
    y[n] ~ normal(beta[g[n]], sigma);
  }
}
```

Instead of `beta`, we include `delta` in the `parameters` block, noted here as the non-centered group intercepts. To impose the deterministic transformation, we include a `transformed parameters` block, which includes both `beta` and the non-centered transformation itself `beta[k] = mu + tau * delta[k]`. Finally, in place of `beta`, the non-centered population model `delta[k] ~ normal(0, 1)` is included in the `model` block.

### Multiple hierarchical regression

Note that the need to break difficult dependencies between the two layers in the hierarchy grows with thinner data or more dimensions introduced with covariates. However, the centered and non-centered parameterizations are inversely related in terms of efficiency; when a centered parameterization will suffice, a non-centered parameterization should underperform and when a non-centered parameterization will suffice, a centered parameterization should underperform.

Conclude with a discussion on efficiency...

- Respond to Stan Discourse reply and include variance parameters (reference Stan User's Guide).
- Plot Delta on tau to see where the divergences are appearing (may need to run longer chains to get better plots).

- Worry about indexing efficiency (i.e., arrays instead of matrices)? Pre-computing X'Beta with a local variable assignment? See Stan User's Guide on Hierarchical Logistic Regression.
- Look at implementation for HMNL?
- Under what conditions does NCP > CP? CP > NCP?
- Adding lower-level covariates `X` -- why does this break NCP?
- Adding upper-level covariates `Z` -- why does this break NCP?

-----

### Marc Dotson

Marc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on [Twitter](https://twitter.com/marcdotson) and [GitHub](https://github.com/marcdotson).
