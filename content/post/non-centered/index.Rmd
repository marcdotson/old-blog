---
title: "Non-centered parameterization for hierarchical models"
author: "Marc Dotson"
date: '2020-03-15'
slug: non-centered
---

In a [previous post](https://www.occasionaldivergences.com/post/stan-hierarchical/), we provided a gentle introduction to building hierarchical Bayesian models in Stan. We quickly ran into divergences (i.e., divergent transitions) when attempting to estimate our model. While hierarchical models have posteriors with [geometry that is difficult to navigate](https://arxiv.org/abs/1312.0906), we were able to initially address the problem by more carefully navigating the posterior with a smaller step size.

However, the posterior geometry of hierarchical models typically requires a non-centered parameterization, expressing the same model in a different way to ease the navigation of the posterior. The goal of this post is to expand on that [previous post](https://www.occasionaldivergences.com/post/stan-hierarchical/) by providing a walkthrough of implementing a non-centered parameterization for a standard hierarchical linear Bayesian model in Stan.

## Non-centered parameterization

The standard centered parameterization for a simple hierarchical regression has a population model and likelihood:

```
beta ~ normal(mu, tau)
y ~ normal(beta, sigma)
```

where we get draws from the posterior distribution of `mu` and `tau`, the population mean and variance, and `beta`, the group-level intercepts.

A non-centered parameterization re-expresses the population model and likelihood for a simple hierarchical regression as:

```
delta ~ normal(0, 1)
beta = mu + tau * delta
y ~ normal(beta, sigma)
```

where we get draws from the posterior distribution of `mu`, `tau`, and `delta`, since `beta` is now a fixed transformation of the other parameters (note that `beta =` instead of `beta ~`). The benefit of a non-centered parameterization (i.e., the inclusion of the intermediate `delta` and the fixed transformation of `beta`) is that difficult dependencies between the two layers in the hierarchy are broken, producing a simpler posterior geometry.

## Simple hierarchical regression

Let's start by demonstrating how to implement a non-centered parameterization for a simple hierarchical regression.

```{stan simple-hlm, output.var="simple_hlm", eval=FALSE}
// Index values and observations.
data {
  int<lower = 1> N;               // Number of observations.
  int<lower = 1> K;               // Number of groups.
  vector[N] y;                    // Vector of observations.
  int<lower = 1, upper = K> g[N]; // Vector of group assignments.
}

// Parameters and hyperparameters.
parameters {
  real mu;                        // Mean of the population model.
  real<lower = 0> tau;            // Variance of the population model.
  vector[K] delta;                // Vector of non-centered group intercepts.
  real<lower = 0> sigma;          // Variance of the likelihood.
}

// Deterministic transformation.
transformed parameters {
  // Vector of transformed group intercepts.
  vector[K] beta;

  // Non-centered parameterization.
  for (k in 1:K) {
    beta[k] = mu + tau * delta[k];
  }
}

// Hierarchical regression.
model {
  // Hyperpriors.
  mu ~ normal(0, 5);
  tau ~ normal(0, 5);

  // Prior.
  sigma ~ normal(0, 5);

  // Non-centered population model and likelihood.
  for (k in 1:K) {
    delta[k] ~ normal(0, 1);
  }
  for (n in 1:N) {
    y[n] ~ normal(beta[g[n]], sigma);
  }
}
```

Instead of `beta`, we include `delta` in the `parameters` block, noted here as the non-centered group intercepts. To impose the deterministic transformation, we include a `transformed parameters` block, which includes both `beta` and the non-centered transformation itself `beta[k] = mu + tau * delta[k]`. Finally, in place of `beta`, the non-centered population model `delta[k] ~ normal(0, 1)` is included in the `model` block.

## Multiple hierarchical regression

Note that the need to break difficult dependencies between the two layers in the hierarchy grows with thinner data or more dimensions introduced with covariates. However, the centered and non-centered parameterizations are inversely related in terms of efficiency; when a centered parameterization will suffice, a non-centered parameterization should underperform and when a non-centered parameterization will suffice, a centered parameterization should underperform.

- Under what conditions does NCP > CP? CP > NCP?
- Look at implementation for HMNL.
- Adding lower-level covariates `X` -- why does this break NCP?
- Add upper-level covariates `Z` -- why does this break NCP?

-----

### Marc Dotson

Marc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on [Twitter](https://twitter.com/marcdotson) and [GitHub](https://github.com/marcdotson).
