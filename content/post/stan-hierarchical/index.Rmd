---
title: Hierarchical models in Stan
author: Marc Dotson
date: '2019-08-27'
slug: stan-hierarchical
categories: []
tags: []
Categories:
  - R
  - Stan
  - Tidyverse
Description: ''
Tags:
  - 
---

Bayesian inference provides an intuitive and self-consistent approach to statistical modeling. In short, you have beliefs about unobserved values (e.g., the impact of price on customer satisfaction) and you use data to update those beliefs. The barrier to using Bayesian inference in practice has never been this intuition -- it's been the [required computation](https://arxiv.org/abs/1701.02434). There has been a great deal of progress on this front, with [Stan](https://mc-stan.org) arguably providing the best solution.

Stan is a probabilistic programming language that provides a general-purpose sampler using Hamiltonian Monte Carlo. In other words, Stan automates the required computation (for many models), allowing you to conduct Bayesian inference by just focusing on model building. This is especially powerful when it comes to utilizing the mainstay of Bayesian inference: hierarchical models.

The goal of this post is to provide a bridge to building hierarchical models using Stan and R. We'll start with a very brief introduction to Stan and build in complexity from there. This post does not provide a general introduction to Bayesian inference or Stan. See:

- Introduction to Bayesian inference
- Introduction to Stan

## Simple regression

Let's start with a simple (non-hierarchical) regression. In a Stan script, which [has native support in RStudio](https://blog.rstudio.com/2018/10/16/rstudio-1-2-preview-stan/), we specify the three required blocks for a Stan model: `data`; `parameters`, the unobserved values we want to estimate; and `model`, both the prior and likelihood.

```{stan regression, output.var = "data", eval = FALSE}
// Index value and observations.
data {
  int<lower = 1> N;  // Number of individuals.
  vector[N] y;       // Vector of observations.
}

// Parameters.
parameters {
  real mu;           // Mean of the regression.
  real<lower=0> tau; // Variance of the regression.
}

// Simple regression.
model {
  // Priors.
  mu ~ normal(0, 5);
  tau ~ cauchy(0, 2.5);

  // Likelihood.
  y ~ normal(mu, tau);
}
```

Imagine this is a model of customer satisfaction where we have `N` individuals, the overall satisfaction `y` from each individual, and we are assuming that satisfaction `y` is distributed `normal`, with a single mean `mu` and variance `tau` to describe customer satisfaction in the population.

Like all Bayesian models, this model is fully generative, so we can also use Stan to generate data according to some assumed parameter values and then use the generated data to test the model, including demonstrating parameter recovery. To do this, we reorganize these three blocks into `data` (which now includes the assumed parameter values) and `generated quantities` blocks.

```{stan generate-data, output.var = "generate_data", eval = FALSE}
// Index and parameter values.
data {
  int<lower = 1> N;  // Number of observations.
  real mu;           // Mean of the regression.
  real<lower=0> tau; // Variance of the regression.
}

// Generate data according to the simple regression.
generated quantities {
  vector[N] y;       // Vector of observations.

  // Generate data.
  for (n in 1:N) {
    y[n] = normal_rng(mu, tau);
  }
}
```

The for loop over `y` in the `generated quantities` block emphasizes the strong assumption of this non-hierarchical model that a single mean `mu` and variance `tau` describe customer satisfaction in the population. We'll return to this shortly.

In an R script, let's load the necessary packages, allow Stan to use as many cores as we have available, specify assumed parameter values, and generate data according to our simple regression.

```{r preamble-generate-data, eval = FALSE}
# Load packages.
library(tidyverse)
library(rstan)
library(bayesplot)
library(tidybayes)

# Set Stan options.
options(mc.cores = parallel::detectCores())

# Specify data and parameter values.
sim_values <- list(
  N = 100, # Number of observations.
  mu = 5,  # Mean of the regression.
  tau = 1  # Variance of the regression.
)

# Generate data.
sim_data <- stan(
  file = here::here("content", "post", "stan-hierarchical", "code", "generate_data.stan"),
  data = sim_values,
  iter = 1,
  chains = 1,
  seed = 42,
  algorithm = "Fixed_param"
)

# Extract simulated data.
sim_y <- extract(sim_data)$y
```

To test our model, we simply specify the generated data inputs as a list, call the regression model from R, and Stan does all the heavy lifting for us.

```{r model-calibration, eval = FALSE}
# Specify data.
data <- list(
  N = length(sim_y),   # Number of individuals.
  y = as.vector(sim_y) # Vector of observations.
)

# Calibrate the model.
fit <- stan(
  file = here::here("content", "post", "stan-hierarchical", "code", "regression.stan"),
  data = data,
  seed = 42
)
```

Note that we aren't calling `generate_data.stan`, we're calling `regression.stan` (the model with the three required blocks).

Once the model has finished running, we can check diagnostics, including checking for divergences, which are unique to using Hamiltonian Monte Carlo.

```{r diagnostics, eval = FALSE}
# Diagnostics.
source(here::here("content", "post", "stan-hierarchical", "code", "stan_utility.R"))
check_all_diagnostics(fit)
```

```
[1] "n_eff / iter looks reasonable for all parameters"
[1] "Rhat looks reasonable for all parameters"
[1] "0 of 4000 iterations ended with a divergence (0%)"
[1] "0 of 4000 iterations saturated the maximum tree depth of 10 (0%)"
[1] "E-FMI indicated no pathological behavior"
```

The diagnostics check out. We can also check the trace plots and, to be thorough, plot the marginal posteriors for our parameter values to confirm that the assumed parameter values used when generating data are within the 95% credible intervals.

```{r trace-plots, eval = FALSE}
# Check trace plots.
fit %>%
  mcmc_trace(
    pars = c("mu", "tau"),
    n_warmup = 500,
    facet_args = list(nrow = 2, labeller = label_parsed)
  )
```

![](figures/mcmc_trace.png)

As suggested by the `Rhat` diagnostic, we have good mixing and clear convergence for both of our model parameters.

```{r marginals, eval = FALSE}
# Recover parameter values.
par_values <- tibble(
  .variable = c("mu", "tau"),
  values = c(sim_values$mu, sim_values$tau),
)

fit %>%
  gather_draws(mu, tau) %>%
  ggplot(aes(x = .value, y = .variable)) +
  geom_halfeyeh(.width = .95) +
  facet_wrap(
    ~ .variable,
    nrow = 2,
    scales = "free"
  ) +
  geom_vline(aes(xintercept = values), par_values, color = "red")
```

![](figures/marginals.png)

The assumed parameter values have been recovered by the model! In summary, we've generated data and demonstrated that the model is working as intended.

## Simple hierarchical regression

So why move to a hierarchical model? A hierarchical model accounts for structure in our data.

If we are estimating the preferences of consumers, we know that consumers preferences are different. This heterogeneity makes the use of a standard linear model incredibly problematic, since that model assumes that preferences are shared across consumers. Just as hierarchical models account for consumer heterogeneity in models of preference, they similarly account for existing hierarchies in other applications and should be considered the default model rather than aggregate models.

Hierarchical models should be our default approach in most applications and Stan removes the barriers to entry for Bayesian computation. However, there are behaviors inherent to hierarchical models that make them somewhat problematic to implement in Stan. This can provide a frustrating mismatch: Providing a way into Bayesian inference only to pull it away again. The motivation for this post is to provide a gentle introduction into implementing hierarchical models in Stan.

## Benefits

- Partial pooling (like shrinking, adaptively between an aggregate model and separate models, which helps avoid overfitting).
- Estimate both individual and population parameters simultaneously.
- Include information in the form of covariates at both individual and population levels.
- Exploit the (frequently present) hierarchical structure in the data.

## Difficulties

The correlations induced within a hierarchical model change the geometry sufficiently as to make their estimation via Hamiltonian Monte Carlo more challenging than the geometry of an aggregate model. Don't throw the baby out with the bathwater -- these same problems exist for other samplers as well, though they might not be as obvious, and HMC promises to still provide a more efficient and complete sampling of the posterior distribution as long as we know how to navigate their implementation.

- Illustrate a hierarchical model (see Michael's hierarchical regression or expand my own tidy Bayesian workflow example into a hierarchical linear model [same thing?]).
- Geometry of hierarchical models.
- Parameterizations (centered vs. non-centered).
- Diagnostics (what's the deal with NaN's? how do we really compute Rhat?).

## Stages

- Simple hierarchical regression, where we have varying-intercepts only.
- Multiple hierarchical regression, where we have varying-intercepts and varying-slopes.
- General hierarchical regression, where we have multiple observations and deal with estimable variance.
