---
title: "Comparing choice model parameterizations"
author: "Marc Dotson"
date: "2020-08-25"
slug: choice-models
---

Choice models are common in marketing and other applications where researchers are interested in understanding both the drivers and trade-offs of choice. Since choice is typically manifest as a non-binary discrete outcome and we care about modeling consumer heterogeneity, a hierarchical Bayesian multinomial logit model is our default specification.

In marketing, choice models are often employed in conjunction with conjoint experiments, a survey-based approach to eliciting preferences where respondents choose from sets of hypothetical product alternatives. Because the conjoint experiment produces repeat observations at the respondent level, [the groups in the hierarchical model](https://www.occasionaldivergences.com/post/stan-hierarchical/) are the respondents themselves.

In this post we will specify a choice model and seek to answer the following question: Does it matter if we use a centered or non-centered parameterization for a hierarchical multinomial logit model? We have previously explored using a [non-centered parameterization](https://www.occasionaldivergences.com/post/non-centered/) for hierarchical regression and even argued that a non-centered parameterization should be our default approach in most applications. However, choice models used in practice employ a centered parameterization almost exclusively.

We will specify and compare performance of a centered parameterization and a non-centered parameterization of a hierarchical multinomial logit model estimated using Hamilton Monte Carlo (HMC) via [Stan](https://mc-stan.org). Stan automates the required computation for many models and HMC is particularly verbose regarding model diagnostics. I am indebted to early tutorials on this topic by [Elea Feit and Kevin Van Horn](https://github.com/ksvanhorn/ART-Forum-2017-Stan-Tutorial) and Jim Savage.

## Centered parameterization

Let's start with a centered parameterization of the hierarchical multinomial logit.

```{stan hmnl-centered, output.var="hmnl_centered", eval=FALSE}

```

We'll save this Stan script as `hmnl_centered.stan`. In the `data` block, we have `R` respondents which each go through `S` choice tasks where each choice task has `A` choice alternatives to choose from. Thus there are `R * S` total observations where each observation takes on a value from `1` to `A`, the chosen alternative. At the observation level, the choice alternatives are defined by `I` covariates. These covariates are the attribute levels of the choice alternatives under consideration. At the population or group level, the respondents are defined by `J` covariates. These covariates are used to explain preference heterogeneity across respondents in the population and improve our ability to predict preferences.

The `data` block also includes the hyperprior values. These define our hyperpriors on the hyperparameters in the population-level model, and can be more easily evaluated when specified as part of the `data` block rather than hard-coded in the `model` block.

Since Bayesian models are generative, we can translate this Stan script into `data` and `generated quantities` blocks and use Stan to generate data for us.

```{stan generate-data, output.var="generate_data", eval=FALSE}

```

We'll save this Stan script as `generate_data.stan`. Note that since `Beta` is a matrix where each row is an observation, the vector output of `multi_normal_rng` is transposed.

In an R script, let's load the necessary packages, allow Stan to use as many cores as we have available, allow for Stan to save compiled code, specify assumed hyperprior values, generate data according to the hierarchical multinomial logit by calling `generate_data.stan`, and estimate our model by calling `hmnl_centered.stan`.

```{r centered-calibration, eval=FALSE}

```

```
Chain 1:  Elapsed Time: 14936.9 seconds (Warm-up)
Chain 1:                15389.6 seconds (Sampling)
Chain 1:                30326.5 seconds (Total)
Chain 1: 
Warning messages:
1: There were 50 divergent transitions after warmup. Increasing adapt_delta above 0.99 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 3950 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
3: There were 4 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
4: Examine the pairs() plot to diagnose sampling problems
 
5: The largest R-hat is NA, indicating chains have not mixed.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#r-hat 
6: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#bulk-ess 
7: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#tail-ess 
```

The HMC diagnostics suggest that the chains haven't mixed (`R-hat` and Bulk Effect Sample Size `ESS`). Despite running the chains for longer, we will get warnings about the reliability of the posterior estimates. Plus a single divergent transition. And at `6000` iterations, it takes `28507.7` seconds total to run the model.

This is enough to demonstrate that the centered parameterization is faulty...

From previous post: Running this model gives us more than 100 divergent transitions. Recall that a divergent transition or divergence is a unique Hamiltonian Monte Carlo diagnostic that identifies problems navigating the posterior distribution. These difficulties with posterior geometry are true regardless of the sampler, but Hamiltonian Monte Carlo makes the issue transparent.

We have already set `adapt_delta = 0.99`, which is the smaller step size we set previously. In order to produce a posterior geometry that can be navigated, we need to reparameterize our model.

## Non-centered parameterization

Reference previous post in explaining the non-centered parameterization, now for HMNL.

```{stan hmnl-noncentered, output.var="hmnl_noncentered", eval=FALSE}

```

Run the model...

```{r noncentered-calibration, eval=FALSE}

```

Diagnostics...

```{r trace-plots-01, eval=FALSE}

```

<!-- ![](Figures/mcmc_trace-gamma.png){width=1000px} -->

Parameter recovery...

```{r marginals-01, eval=FALSE}

```

<!-- ![](Figures/marginals-gamma.png){width=1000px} -->

## Comparison

- Okay, but *how* wrong is it? Use Adam's referenced paper.
- Computation time.
- Model fit.
- Comparison across *many* simulated datasets.

## Final thoughts

...

-----

### Marc Dotson

Marc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on [Twitter](https://twitter.com/marcdotson) and [GitHub](https://github.com/marcdotson).
