---
title: "Coding discrete variables"
author: "Marc Dotson"
date: "2021-10-22"
slug: discrete-coding
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>There are a number of ways to code discrete (i.e., categorical) variables, with different coding strategies better suited for specific use cases. Additionally, there are certain practices that are questionable, such as specifying a brand intercept (i.e., index coding the brand attribute) in a hierarchical multinomial logit model where an intercept isn’t identified. Finally, there is a question of using contrasts of marginal posteriors to answer hypotheses and how those contrasts are or are not impacted by the coding used.</p>
<p>Let’s start with a simple model and work up to choice models (where we can use what was detailed in the previous post on choice model parameterizations).</p>
<div id="flat-regression" class="section level2">
<h2>Flat regression</h2>
<p>Generate data that follows index coding so its easy to extrapolate to other coding strategies.</p>
<pre class="stan"><code>// Index and parameter values.
data {
  int&lt;lower = 1&gt; N;    // Number of observations.
  int&lt;lower = 1&gt; I;    // Number of covariates.
  matrix[N, I] X;      // Matrix of covariates.

  vector[I] beta;      // Vector of slopes.
  real&lt;lower = 0&gt; tau; // Variance of the regression.
}

// Generate data according to the simple regression.
generated quantities {
  // Vector of observations.
  vector[N] y;

  // Generate data.
  for (n in 1:N) {
    y[n] = normal_rng(X[n,] * beta, tau);
  }
}</code></pre>
<p>Call <code>generate_flat_data</code> from R.</p>
<pre class="r"><code># Load packages.
library(tidyverse)
library(cmdstanr)
library(posterior)
library(bayesplot)
library(tidybayes)

# Set the simulation seed.
set.seed(42)

# Specify data and parameter values.
sim_values &lt;- list(
  N = 50,                    # Number of observations.
  I = 5,                     # Number of covariates.
  J = c(2, 3),               # Number of levels for each discrete variable.
  beta = c(1, -4, 6, 3, -2), # Vector of slopes.
  tau = 1                    # Variance of the regression.
)

# Matrix of covariates.
sim_X &lt;- matrix(data = 0, nrow = sim_values$N, ncol = (sim_values$I))
for (n in 1:sim_values$N) {
  temp_X &lt;- NULL
  for (j in 1:length(sim_values$J)) {
    temp_J &lt;- rep(0, sim_values$J[j])
    temp_J[sample(seq(1, (sim_values$J[j])), 1)] &lt;- 1
    temp_X &lt;- c(temp_X, temp_J)
  }
  sim_X[n,] &lt;- temp_X
}
sim_values$X &lt;- sim_X

# Compile the model for generating data.
generate_flat_data &lt;- cmdstan_model(
  stan_file = here::here(&quot;content&quot;, &quot;post&quot;, &quot;discrete-coding&quot;, &quot;Code&quot;, &quot;generate_flat_data.stan&quot;),
  dir = here::here(&quot;content&quot;, &quot;post&quot;, &quot;discrete-coding&quot;, &quot;Code&quot;, &quot;Compiled&quot;)
)

# Generate data.
sim_data &lt;- generate_flat_data$sample(
  data = sim_values,
  chains = 1,
  iter_sampling = 1,
  seed = 42,
  fixed_param = TRUE
)

# Extract generated data.
sim_y &lt;- sim_data$draws(variables = &quot;y&quot;, format = &quot;draws_list&quot;) %&gt;%
  pluck(1) %&gt;%
  flatten_dbl()</code></pre>
<div id="dummy-coding" class="section level3">
<h3>Dummy coding</h3>
<p>Also known as indicator coding, dummy coding is likely the most common way to code discrete variables. Here we include an intercept and drop the first level of each of the discrete variables. Here’s a flat regression using dummy coding.</p>
<pre class="stan"><code>// Index value and observations.
data {
  int&lt;lower = 1&gt; N;    // Number of observations.
  int&lt;lower = 1&gt; I;    // Number of covariates.
  vector[N] y;         // Vector of observations.
  matrix[N, I] X;      // Matrix of covariates.
}

// Parameters.
parameters {
  real alpha;          // Intercept.
  vector[I] beta;      // Vector of slopes.
  real&lt;lower = 0&gt; tau; // Variance of the regression.
}

// Regression.
model {
  // Priors.
  alpha ~ normal(0, 5);
  for (i in 1:I) {
    beta[i] ~ normal(0, 5);
  }
  tau ~ normal(0, 5);

  // Likelihood.
  for (n in 1:N) {
    y[n] ~ normal(alpha + X[n,] * beta, tau);
  }
}</code></pre>
<p>Let’s call <code>flat_regression_dummy</code> from R.</p>
<pre class="r"><code># Specify data.
data &lt;- list(
  N = length(sim_y),    # Number of observations.
  I = ncol(sim_X) - 2,  # Number of covariates.
  y = sim_y,            # Vector of observations.
  X = sim_X[, -c(1, 3)] # Matrix of covariates.
)

# Compile the model.
flat_regression_dummy &lt;- cmdstan_model(
  stan_file = here::here(&quot;content&quot;, &quot;post&quot;, &quot;discrete-coding&quot;, &quot;Code&quot;, &quot;flat_regression_dummy.stan&quot;),
  dir = here::here(&quot;content&quot;, &quot;post&quot;, &quot;discrete-coding&quot;, &quot;Code&quot;, &quot;Compiled&quot;)
)

# Fit the model.
fit_dummy &lt;- flat_regression_dummy$sample(
  data = data,
  chains = 4,
  parallel_chains = 4,
  seed = 42
)</code></pre>
<p>We can’t recover the parameters values we used when simulating the data. Dummy coding is equivalent to specifying each included level as a contrast with the reference level. We can see that by computing contrasts and comparing it to the contrasted true parameter values.</p>
<pre class="r"><code># Extract draws and compare contrasts.
contrast_values &lt;- tibble(
  .variable = str_c(&quot;contrast&quot;, 1:(sim_values$I - length(sim_values$J))),
  values = c(
    # First discrete variable.
    sim_values$beta[2] - sim_values$beta[1],
    # Second discrete variable.
    sim_values$beta[4] - sim_values$beta[3],
    sim_values$beta[5] - sim_values$beta[3]
  )
)
fit_dummy$draws(variables = &quot;beta&quot;, format = &quot;draws_df&quot;) %&gt;%
  mutate_variables(
    contrast1 = `beta[1]`,
    contrast2 = `beta[2]`,
    contrast3 = `beta[3]`
  ) %&gt;%
  gather_draws(contrast1, contrast2, contrast3) %&gt;%
  ggplot(aes(y = .variable, x = .value)) +
  stat_histinterval() +
  geom_vline(aes(xintercept = values), contrast_values, color = &quot;red&quot;) +
  facet_wrap(~ .variable, scales = &quot;free&quot;, ncol = 1)</code></pre>
<p><img src="Figures/flat-contrasts-dummy.png" width="750" /></p>
</div>
<div id="index-coding" class="section level3">
<h3>Index coding</h3>
<p>For index coding we <em>don’t</em> include an intercept and <em>don’t</em> include any reference levels. By not including reference levels, the intercept is implied. Here’s a flat regression using index coding.</p>
<pre class="stan"><code>// Index value and observations.
data {
  int&lt;lower = 1&gt; N;    // Number of observations.
  int&lt;lower = 1&gt; I;    // Number of covariates.
  vector[N] y;         // Vector of observations.
  matrix[N, I] X;      // Matrix of covariates.
}

// Parameters.
parameters {
  vector[I] beta;      // Vector of slopes.
  real&lt;lower = 0&gt; tau; // Variance of the regression.
}

// Regression.
model {
  // Priors.
  for (i in 1:I) {
    beta[i] ~ normal(0, 5);
  }
  tau ~ normal(0, 5);

  // Likelihood.
  for (n in 1:N) {
    y[n] ~ normal(X[n,] * beta, tau);
  }
}</code></pre>
<p>And we call <code>flat-regression-index</code> from R.</p>
<pre class="r"><code># Specify data.
data &lt;- list(
  N = length(sim_y),   # Number of observations.
  I = ncol(sim_X),     # Number of covariates.
  y = sim_y,           # Vector of observations.
  X = sim_X            # Matrix of covariates.
)

# Compile the model.
flat_regression_index &lt;- cmdstan_model(
  stan_file = here::here(&quot;content&quot;, &quot;post&quot;, &quot;discrete-coding&quot;, &quot;Code&quot;, &quot;flat_regression_index.stan&quot;),
  dir = here::here(&quot;content&quot;, &quot;post&quot;, &quot;discrete-coding&quot;, &quot;Code&quot;, &quot;Compiled&quot;)
)

# Fit the model.
fit_index &lt;- flat_regression_index$sample(
  data = data,
  chains = 4,
  parallel_chains = 4,
  seed = 42
)</code></pre>
<p>With index coding, the parameter estimates don’t produce an implied contrast, so we’ll have to contrast them directly. Here we re-produce the same plot as before and compare it to the contrasted true parameter values.</p>
<pre class="r"><code># Extract draws and compare contrasts.
contrast_values &lt;- tibble(
  .variable = str_c(&quot;contrast&quot;, 1:(sim_values$I - length(sim_values$J))),
  values = c(
    # First discrete variable.
    sim_values$beta[2] - sim_values$beta[1],
    # Second discrete variable.
    sim_values$beta[4] - sim_values$beta[3],
    sim_values$beta[5] - sim_values$beta[3]
  )
)
fit_index$draws(variables = c(&quot;beta&quot;), format = &quot;draws_df&quot;) %&gt;%
  mutate_variables(
    contrast1 = `beta[2]` - `beta[1]`,
    contrast2 = `beta[4]` - `beta[3]`,
    contrast3 = `beta[5]` - `beta[3]`
  ) %&gt;%
  gather_draws(contrast1, contrast2, contrast3) %&gt;%
  ggplot(aes(y = .variable, x = .value)) +
  stat_histinterval() +
  geom_vline(aes(xintercept = values), contrast_values, color = &quot;red&quot;) +
  facet_wrap(~ .variable, scales = &quot;free&quot;, ncol = 1)</code></pre>
<p><img src="Figures/flat-contrasts-dummy.png" width="750" /></p>
</div>
<div id="effects-coding" class="section level3">
<h3>Effects coding</h3>
<pre class="stan"><code></code></pre>
</div>
</div>
<div id="hierarchical-regression" class="section level2">
<h2>Hierarchical regression</h2>
</div>
<div id="choice-model" class="section level2">
<h2>Choice model</h2>
<p>And the peculiar case of intercepts and brand intercepts…if intercepts aren’t identified, it could be the prior alone that is providing a sensible posterior. To test this, let’s use a uniform prior.</p>
</div>
<div id="final-thoughts" class="section level2">
<h2>Final thoughts</h2>
<hr />
<div id="marc-dotson" class="section level3">
<h3>Marc Dotson</h3>
<p>Marc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on <a href="https://twitter.com/marcdotson">Twitter</a> and <a href="https://github.com/marcdotson">GitHub</a>.</p>
</div>
</div>
