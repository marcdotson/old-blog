---
title: "Coding discrete variables"
author: "Marc Dotson"
date: "2021-10-01"
slug: discrete-coding
---

There are a number of ways to code discrete (i.e., categorical) variables, with different coding strategies better suited for specific use cases. Additionally, there are certain practices that are questionable, such as specifying a brand intercept (i.e., index coding the brand attribute) in a hierarchical multinomial logit model where an intercept isn't identified. Finally, there is a question of using contrasts of marginal posteriors to answer hypotheses and how those contrasts are or are not impacted by the coding used.

Let's start with a simple model and work up to choice models (where we can use what was detailed in the previous post on choice model parameterizations).

## Flat regression

Generate data that follows index coding so its easy to extrapolate to other specific coding.

```{stan generate-flat-data, eval=FALSE}
// Index and parameter values.
data {
  int<lower = 1> N;    // Number of observations.
  int<lower = 1> I;    // Number of covariates.
  matrix[N, I] X;      // Matrix of covariates.

  vector[I] beta;      // Vector of slopes.
  real<lower = 0> tau; // Variance of the regression.
}

// Generate data according to the simple regression.
generated quantities {
  // Vector of observations.
  vector[N] y;

  // Generate data.
  for (n in 1:N) {
    y[n] = normal_rng(X[n,] * beta, tau);
  }
}
```


```{r generate-flat-data, eval=FALSE}

```

### Dummy coding

Also known as indicator coding is likely the most common way to code discrete variables.

```{stan flat-regression-dummy, eval=FALSE}
// Index value and observations.
data {
  int<lower = 1> N;    // Number of observations.
  int<lower = 1> I;    // Number of covariates.
  vector[N] y;         // Vector of observations.
  matrix[N, I] X;      // Matrix of covariates.
}

// Parameters.
parameters {
  real alpha;          // Intercept.
  vector[I] beta;      // Vector of slopes.
  real<lower = 0> tau; // Variance of the regression.
}

// Regression.
model {
  // Priors.
  alpha ~ normal(0, 5);
  for (i in 1:I) {
    beta[i] ~ normal(0, 5);
  }
  tau ~ normal(0, 5);

  // Likelihood.
  for (n in 1:N) {
    y[n] ~ normal(alpha + X[n,] * beta, tau);
  }
}

```


```{r flat-contrasts-dummy, eval=FALSE}

```

### Index coding

Separate indicators in place of an intercept.

```{stan flat-regression-index, eval=FALSE}

```

```{r flat-contrasts-index, eval=FALSE}

```

### Effects coding

```{stan flat-regression-effects, eval=FALSE}

```

```{r flat-contrasts-effects, eval=FALSE}

```

## Hierarchical regression

## Choice model

And the peculiar case of intercepts and brand intercepts...if intercepts aren't identified, it could be the prior alone that is providing a sensible posterior. To test this, let's use a uniform prior.

## Final thoughts

-----

### Marc Dotson

Marc is an assistant professor of marketing at the BYU Marriott School of Business. He graduated with an MSc from The London School of Economics and Political Science in 2009 and a PhD from The Ohio State University in 2016. His research interests include Bayesian inference, predictive modeling, consumer preference heterogeneity, and unstructured data. Marc teaches marketing analytics. You can find him on [Twitter](https://twitter.com/marcdotson) and [GitHub](https://github.com/marcdotson).
